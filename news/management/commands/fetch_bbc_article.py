from django.core.management.base import BaseCommand
from django.utils import timezone
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from news.models import Article, Category, Tag
from django.contrib.auth import get_user_model
import logging
import random
import time

logger = logging.getLogger(__name__)


class Command(BaseCommand):
    help = '–ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏'

    def handle(self, *args, **options):
        try:
            source = {
                'name': '–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏',
                'url': 'https://ria.ru',
                'category': '–†–æ—Å—Å–∏—è',
                'selectors': {
                    'articles': [
                        'a.list-item__title',  # –û—Å–Ω–æ–≤–Ω–æ–π —Å–µ–ª–µ–∫—Ç–æ—Ä
                        'div.list-item__content a',  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä
                        'a.cell-list__item-link'  # –î–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π —Å–∞–π—Ç–∞
                    ],
                    'title': 'h1.article__title',
                    'content': 'div.article__text',
                    'date': 'div.article__info-date-time',
                    'image': 'div.article__photo img'
                },
                'delay': 3
            }

            self.stdout.write("üîç –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏...")

            # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Å —Ä–æ—Ç–∞—Ü–∏–µ–π User-Agent
            soup = self.fetch_page(source['url'])
            if not soup:
                return

            # 2. –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
            article_links = self.find_articles(soup, source)
            if not article_links:
                self.stdout.write("‚Ñπ –°—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
                self.stdout.write("- –ò–∑–º–µ–Ω–∏–ª–∞—Å—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∞–π—Ç–∞")
                self.stdout.write("- –ù—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å —Å–µ–ª–µ–∫—Ç–æ—Ä—ã")
                with open('ria_debug.html', 'w', encoding='utf-8') as f:
                    f.write(soup.prettify())
                self.stdout.write("‚ö† –°–æ—Ö—Ä–∞–Ω—ë–Ω HTML –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: ria_debug.html")
                return

            # 3. –í—ã–±–æ—Ä –∏ –ø–∞—Ä—Å–∏–Ω–≥ —Å–ª—É—á–∞–π–Ω–æ–π —Å—Ç–∞—Ç—å–∏
            article_url = urljoin(source['url'], random.choice(article_links)['href'])
            time.sleep(source['delay'])

            article_data = self.parse_article(article_url, source)
            if article_data:
                self.save_article(article_data, source)
                self.stdout.write(self.style.SUCCESS(
                    f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {article_data['title'][:60]}..."
                ))
                self.stdout.write(f"üîó –°—Å—ã–ª–∫–∞: {article_url}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞: {str(e)}", exc_info=True)
            self.stdout.write(self.style.ERROR(
                f"‚ùå –û—à–∏–±–∫–∞: {str(e)}"
            ))

    def fetch_page(self, url):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å —Ä–æ—Ç–∞—Ü–∏–µ–π –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)',
            'Mozilla/5.0 (X11; Linux x86_64)'
        ]

        headers = {
            'User-Agent': random.choice(user_agents),
            'Accept-Language': 'ru-RU,ru;q=0.9',
            'Referer': 'https://www.google.com/',
            'DNT': '1'
        }

        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()

            if response.status_code != 200:
                raise ValueError(f"HTTP —Å—Ç–∞—Ç—É—Å: {response.status_code}")

            return BeautifulSoup(response.text, 'html.parser')

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {str(e)}")
            return None

    def find_articles(self, soup, source):
        """–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º"""
        articles = []

        for selector in source['selectors']['articles']:
            found = soup.select(selector)
            if found:
                articles.extend(found)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã—Ö URL
        filtered = []
        for a in articles:
            href = a.get('href', '')
            if href and not any(d in href for d in ['/embed/', '-print.html', '/search/']):
                filtered.append(a)

        return filtered

    def parse_article(self, url, source):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç—å–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""
        soup = self.fetch_page(url)
        if not soup:
            return None

        try:
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title = soup.select_one(source['selectors']['title'])
            if not title:
                raise ValueError("–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω")

            # –ö–æ–Ω—Ç–µ–Ω—Ç
            content_blocks = soup.select(source['selectors']['content'])
            if not content_blocks:
                raise ValueError("–ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")

            # –î–∞—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            date = soup.select_one(source['selectors']['date'])
            pub_date = self.parse_date(date.text.strip()) if date else timezone.now()

            return {
                'title': title.text.strip(),
                'content': '\n\n'.join(p.text.strip() for p in content_blocks if p.text.strip()),
                'date': pub_date,
                'url': url,
                'image_url': self.get_image_url(soup, source)
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏: {str(e)}")
            return None

    def get_image_url(self, soup, source):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ URL –≥–ª–∞–≤–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        img = soup.select_one(source['selectors']['image'])
        return urljoin(source['url'], img['src']) if img and img.get('src') else None

    def parse_date(self, date_str):
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        try:
            # –ü—Ä–∏–º–µ—Ä: "02:30 17.10.2023"
            from datetime import datetime
            return datetime.strptime(date_str, '%H:%M %d.%m.%Y')
        except:
            return timezone.now()

    def save_article(self, data, source):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        if Article.objects.filter(title=data['title'][:255]).exists():
            logger.info(f"–°—Ç–∞—Ç—å—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {data['title'][:50]}...")
            return

        category, _ = Category.objects.get_or_create(
            name=source['category'],
            defaults={'name': source['category']}
        )

        article = Article.objects.create(
            title=data['title'][:255],
            content=data['content'][:10000],
            publication_date=data['date'],
            category=category,
            author=get_user_model().objects.first(),
            status=True,
            is_active=True,
            level=Article.Level.TOP,
            original_url=data['url']
        )

        tags = ["–ù–æ–≤–æ—Å—Ç–∏", "–†–ò–ê", source['category']]
        for tag_name in tags:
            tag, _ = Tag.objects.get_or_create(name=tag_name[:30])
            article.tags.add(tag)